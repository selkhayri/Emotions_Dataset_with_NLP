{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "from tensorflow.sparse import  SparseTensor, to_dense\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the size of the columns to be able to view more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display column width to 500\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0                                                                                       i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2                                                              im grabbing a minute to post i feel greedy wrong   \n",
       "3                  i am ever feeling nostalgic about the fireplace i will know that it is still on the property   \n",
       "4                                                                                          i am feeling grouchy   \n",
       "\n",
       "     label  \n",
       "0  sadness  \n",
       "1  sadness  \n",
       "2    anger  \n",
       "3     love  \n",
       "4    anger  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>anger</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Read the csv files\n",
    "\n",
    "data_train = pd.read_csv(\"train.txt\", sep=';', header=None)\n",
    "data_val = pd.read_csv(\"val.txt\", sep=';', header=None)\n",
    "data_test = pd.read_csv(\"test.txt\", sep=';', header=None)\n",
    "\n",
    "# Set the column headers of the dataframes\n",
    "\n",
    "data_train.columns = ['text', 'label']\n",
    "data_val.columns = ['text', 'label']\n",
    "data_test.columns = ['text', 'label']\n",
    "\n",
    "# Combine all three dataframes into one\n",
    "\n",
    "data = pd.concat((data_train,data_val,data_test),axis=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    20000\n",
       "Name: punct_count, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "### Create a punct_count column to contain the number of punctuation characters in each line of data\n",
    "data[\"punct_count\"] = len([char for char in data[\"text\"] if char in string.punctuation])\n",
    "\n",
    "### Get the frequency of the number of punctuation counts \n",
    "data[\"punct_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['text_tokenized'] = data[\"text\"].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0                                                                                       i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2                                                              im grabbing a minute to post i feel greedy wrong   \n",
       "3                  i am ever feeling nostalgic about the fireplace i will know that it is still on the property   \n",
       "4                                                                                          i am feeling grouchy   \n",
       "\n",
       "     label  punct_count  \\\n",
       "0  sadness            0   \n",
       "1  sadness            0   \n",
       "2    anger            0   \n",
       "3     love            0   \n",
       "4    anger            0   \n",
       "\n",
       "                                                                                                                       text_tokenized  \n",
       "0                                                                                                        [i, didnt, feel, humiliated]  \n",
       "1  [i, can, go, from, feeling, so, hopeless, to, so, damned, hopeful, just, from, being, around, someone, who, cares, and, is, awake]  \n",
       "2                                                                         [im, grabbing, a, minute, to, post, i, feel, greedy, wrong]  \n",
       "3                     [i, am, ever, feeling, nostalgic, about, the, fireplace, i, will, know, that, it, is, still, on, the, property]  \n",
       "4                                                                                                           [i, am, feeling, grouchy]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>punct_count</th>\n      <th>text_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>sadness</td>\n      <td>0</td>\n      <td>[i, didnt, feel, humiliated]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n      <td>sadness</td>\n      <td>0</td>\n      <td>[i, can, go, from, feeling, so, hopeless, to, so, damned, hopeful, just, from, being, around, someone, who, cares, and, is, awake]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>anger</td>\n      <td>0</td>\n      <td>[im, grabbing, a, minute, to, post, i, feel, greedy, wrong]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n      <td>love</td>\n      <td>0</td>\n      <td>[i, am, ever, feeling, nostalgic, about, the, fireplace, i, will, know, that, it, is, still, on, the, property]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>anger</td>\n      <td>0</td>\n      <td>[i, am, feeling, grouchy]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "joy         6761\n",
       "sadness     5797\n",
       "anger       2709\n",
       "fear        2373\n",
       "love        1641\n",
       "surprise     719\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Additional Stop words\n",
    "stop_words = [\"arent\", \"cant\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"hed\", \"hell\", \"hes\", \"Id\", \"Ill\", \"Im\", \"Ive\", \"isnt\", \"lets\", \"mightnt\", \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldnt\", \"thats\", \"theres\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"wed\", \"were\", \"weve\", \"werent\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"wheres\", \"whos\", \"wholl\", \"whore\", \"whos\", \"whove\", \"wont\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0                                                                                       i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2                                                              im grabbing a minute to post i feel greedy wrong   \n",
       "3                  i am ever feeling nostalgic about the fireplace i will know that it is still on the property   \n",
       "4                                                                                          i am feeling grouchy   \n",
       "\n",
       "     label  punct_count  \\\n",
       "0  sadness            0   \n",
       "1  sadness            0   \n",
       "2    anger            0   \n",
       "3     love            0   \n",
       "4    anger            0   \n",
       "\n",
       "                                                                                                                       text_tokenized  \\\n",
       "0                                                                                                        [i, didnt, feel, humiliated]   \n",
       "1  [i, can, go, from, feeling, so, hopeless, to, so, damned, hopeful, just, from, being, around, someone, who, cares, and, is, awake]   \n",
       "2                                                                         [im, grabbing, a, minute, to, post, i, feel, greedy, wrong]   \n",
       "3                     [i, am, ever, feeling, nostalgic, about, the, fireplace, i, will, know, that, it, is, still, on, the, property]   \n",
       "4                                                                                                           [i, am, feeling, grouchy]   \n",
       "\n",
       "                                          text_tokenized_nostop  \n",
       "0                                               feel humiliated  \n",
       "1  go feeling hopeless damned hopeful around someone care awake  \n",
       "2                     im grabbing minute post feel greedy wrong  \n",
       "3          ever feeling nostalgic fireplace know still property  \n",
       "4                                               feeling grouchy  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>punct_count</th>\n      <th>text_tokenized</th>\n      <th>text_tokenized_nostop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>sadness</td>\n      <td>0</td>\n      <td>[i, didnt, feel, humiliated]</td>\n      <td>feel humiliated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n      <td>sadness</td>\n      <td>0</td>\n      <td>[i, can, go, from, feeling, so, hopeless, to, so, damned, hopeful, just, from, being, around, someone, who, cares, and, is, awake]</td>\n      <td>go feeling hopeless damned hopeful around someone care awake</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>anger</td>\n      <td>0</td>\n      <td>[im, grabbing, a, minute, to, post, i, feel, greedy, wrong]</td>\n      <td>im grabbing minute post feel greedy wrong</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n      <td>love</td>\n      <td>0</td>\n      <td>[i, am, ever, feeling, nostalgic, about, the, fireplace, i, will, know, that, it, is, still, on, the, property]</td>\n      <td>ever feeling nostalgic fireplace know still property</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>anger</td>\n      <td>0</td>\n      <td>[i, am, feeling, grouchy]</td>\n      <td>feeling grouchy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "def remove_stopwords(word_list):\n",
    "    return \" \".join([WordNetLemmatizer().lemmatize(word) for word in word_list if word not in stopwords and word not in stop_words])\n",
    "\n",
    "data[\"text_tokenized_nostop\"] = data[\"text_tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text using the TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TfidfVectorizer and fit it to the entire text corpus (training, validation, test)\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf_vect = tfidf_vect.fit_transform(data.iloc[:data.shape[0],:][\"text_tokenized_nostop\"])\n",
    "\n",
    "# Get the features (tokens) that were identified by the vectorizer\n",
    "X_features = pd.DataFrame(X_tfidf_vect.toarray())\n",
    "\n",
    "# Scale the features \n",
    "X_scaler = StandardScaler()\n",
    "X_features_scaled = X_scaler.fit_transform(X_features)\n",
    "X = X_features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0    1    2    3    4    5\n",
       "0      0.0  0.0  0.0  0.0  1.0  0.0\n",
       "1      0.0  0.0  0.0  0.0  1.0  0.0\n",
       "2      1.0  0.0  0.0  0.0  0.0  0.0\n",
       "3      0.0  0.0  0.0  1.0  0.0  0.0\n",
       "4      1.0  0.0  0.0  0.0  0.0  0.0\n",
       "...    ...  ...  ...  ...  ...  ...\n",
       "19995  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "19996  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "19997  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "19998  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "19999  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[20000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(data[[\"label\"]])\n",
    "pd.DataFrame(y.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sparse matrix to SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the training, validation, and test datasets from the larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = X[:data_train.shape[0],:]\n",
    "y_train = y[:data_train.shape[0]]\n",
    "\n",
    "# Validation\n",
    "X_val = X[data_train.shape[0]:data_train.shape[0] + data_val.shape[0],:]\n",
    "y_val = y[data_train.shape[0]:data_train.shape[0] + data_val.shape[0]]\n",
    "\n",
    "# Test\n",
    "X_test = X[data_train.shape[0] + data_val.shape[0]:data_train.shape[0]\n",
    "              + data_val.shape[0] \n",
    "              + data_test.shape[0],:]\n",
    "y_test = y[data_train.shape[0] + data_val.shape[0]:data_train.shape[0]\n",
    "              + data_val.shape[0] \n",
    "              + data_test.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert target dataframes to dense tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_dense(convert_sparse_matrix_to_sparse_tensor(y_train))\n",
    "y_val = to_dense(convert_sparse_matrix_to_sparse_tensor(y_val))\n",
    "y_test = to_dense(convert_sparse_matrix_to_sparse_tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential()\n",
    "seq.add(Dense(units=1000,activation=\"relu\",input_dim=X_train.shape[1]))\n",
    "seq.add(Dropout(0.2))\n",
    "seq.add(Dense(units=50,activation=\"relu\"))\n",
    "seq.add(Dropout(0.2))\n",
    "seq.add(Dense(units=6,activation=\"softmax\"))\n",
    "\n",
    "seq.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the sequential model to the training, validating with the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 1.2303 - accuracy: 0.5664 - val_loss: 0.7490 - val_accuracy: 0.7365\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 144s 288ms/step - loss: 0.5942 - accuracy: 0.7986 - val_loss: 0.7119 - val_accuracy: 0.7585\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 144s 288ms/step - loss: 0.4427 - accuracy: 0.8659 - val_loss: 0.7152 - val_accuracy: 0.7765\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 145s 290ms/step - loss: 0.3653 - accuracy: 0.9014 - val_loss: 0.7803 - val_accuracy: 0.7480\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 101s 202ms/step - loss: 0.3288 - accuracy: 0.9181 - val_loss: 0.8095 - val_accuracy: 0.7700\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2861 - accuracy: 0.9344 - val_loss: 0.8164 - val_accuracy: 0.7700\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2721 - accuracy: 0.9438 - val_loss: 0.8724 - val_accuracy: 0.7770\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2829 - accuracy: 0.9466 - val_loss: 0.9614 - val_accuracy: 0.8050\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2496 - accuracy: 0.9514 - val_loss: 1.2769 - val_accuracy: 0.7880\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2634 - accuracy: 0.9534 - val_loss: 1.3235 - val_accuracy: 0.7350\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.2706 - accuracy: 0.9554 - val_loss: 1.1312 - val_accuracy: 0.7950\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3188 - accuracy: 0.9538 - val_loss: 1.2244 - val_accuracy: 0.7715\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3249 - accuracy: 0.9571 - val_loss: 1.2563 - val_accuracy: 0.7490\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3331 - accuracy: 0.9504 - val_loss: 1.1914 - val_accuracy: 0.7685\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3326 - accuracy: 0.9504 - val_loss: 1.2091 - val_accuracy: 0.7220\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3133 - accuracy: 0.9499 - val_loss: 1.2005 - val_accuracy: 0.7645\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 62s 125ms/step - loss: 0.3541 - accuracy: 0.9501 - val_loss: 1.3110 - val_accuracy: 0.7805\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 62s 125ms/step - loss: 0.3070 - accuracy: 0.9529 - val_loss: 1.3920 - val_accuracy: 0.7835\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 62s 125ms/step - loss: 0.3808 - accuracy: 0.9496 - val_loss: 2.0954 - val_accuracy: 0.7610\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 62s 125ms/step - loss: 0.3030 - accuracy: 0.9511 - val_loss: 1.4284 - val_accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# fit_model = seq.fit(X_train,y_train,epochs=20)\n",
    "fit_model = seq.fit(X_train,y_train,epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1502"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the labels of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = seq.predict(X_test,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the raw predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[5.24492789e-05, 8.82536533e-09, 5.33850698e-06, 4.33755115e-10,\n",
       "        9.99942183e-01, 4.38483222e-10],\n",
       "       [7.29042540e-06, 4.23976021e-10, 1.02398783e-08, 3.59674821e-14,\n",
       "        9.99992728e-01, 7.03856935e-15],\n",
       "       [3.11584372e-05, 1.61219832e-05, 3.02527472e-03, 2.68819256e-09,\n",
       "        9.96927440e-01, 3.05058667e-09],\n",
       "       ...,\n",
       "       [2.66448613e-22, 1.20579375e-14, 1.00000000e+00, 1.24946702e-08,\n",
       "        1.14732930e-13, 2.68466977e-20],\n",
       "       [7.73647257e-11, 4.24790613e-07, 9.99984264e-01, 1.51705044e-05,\n",
       "        1.28426521e-07, 1.29354183e-09],\n",
       "       [2.16530892e-03, 5.52239120e-01, 9.49000847e-03, 8.11773643e-05,\n",
       "        1.72783539e-01, 2.63240933e-01]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert lists of floats into lists of ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int(fl):\n",
    "    int_2d = []\n",
    "    for row in fl:\n",
    "        i_row = []\n",
    "        for x in row:\n",
    "            i_row.append(int(x+0.5))\n",
    "        int_2d.append(i_row)\n",
    "    \n",
    "    return np.array(int_2d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the elements of the prediction dataframe to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = float_to_int(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the test portion of the larger dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_nparray = y[data_train.shape[0] + data_val.shape[0]:data_train.shape[0]\n",
    "              + data_val.shape[0] \n",
    "              + data_test.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "y[data_train.shape[0] + data_val.shape[0]:data_train.shape[0]\\\n",
    "              + data_val.shape[0] \n",
    "              + data_test.shape[0]:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<2000x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "y_test_nparray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the accuracy of the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9255833333333333"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "(y_test_pred == y_test_nparray).sum()/(y_test_pred.shape[0] * y_test_pred.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}